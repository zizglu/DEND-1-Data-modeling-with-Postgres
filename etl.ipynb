{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "- [ETL Processes tuning](#ETL-Processes-tuning)\n",
    "    - [Common elements](#Common-elements)\n",
    "- [Process song_data](#Process-%3Ccode%3Esong_data%3C%2Fcode%3E)\n",
    "    - [Do some data analysis & quality check](#Do-some-data-analysis-%26amp%3B-quality-check)\n",
    "        - [Loading all json in a dataframe](#Loading-all-json-in-a-dataframe)\n",
    "        - [Looking for duplicates](#Looking-for-duplicates)\n",
    "        - [Choosing SQL data type & constraint](#Choosing-SQL-data-type-%26amp%3B-constraint) \n",
    "    - [#1 songs Table](#%231%3A-%3Ccode%3Esongs%3C%2Fcode%3E-Table)\n",
    "        - [Extract Data for Songs Table](#Extract-Data-for-Songs-Table)\n",
    "        - [Insert (only one) Record into Song Table](#Insert-(only-one)-Record-into-Song-Table)\n",
    "        - [Insert all Record into Song Table in one transaction](#Insert-all-Record-into-Song-Table-in-one-transaction)\n",
    "            - [The morgify() way](#The-morgify()-way)\n",
    "            - [The COPY way](#The-COPY-way) \n",
    "    - [#2: artists Table](#%232%3A-%3Ccode%3Eartists%3C%2Fcode%3E-Table)\n",
    "        - [Extract Data for Artists Table](#Extract-Data-for-Artists-Table)\n",
    "        - [Removing the duplicates](#Removing-the-duplicates)\n",
    "        - [Insert Record into Artist Table](#Insert-Record-into-Artist-Table)\n",
    "        - [Insert data with copy_from](#Insert-data-with-copy_from)\n",
    "    - [Lessons learned with the json files](#Lessons-learned-with-the-json-files)\n",
    "- [Process log_data](#Process-%3Ccode%3Elog_data%3C%2Fcode%3E)\n",
    "    - [Again... do some data analysis & quality check](#Again...-do-some-data-analysis-%26amp%3B-quality-check)\n",
    "    - [#3: time Table](#%233%3A-%3Ccode%3Etime%3C%2Fcode%3E-Table)\n",
    "        - [Extract Data for Time Table](#Extract-Data-for-Time-Table)\n",
    "        - [Choosing the SQL data type for time table and adding a primary key](#Choosing-the-SQL-data-type-for-time-table-and-adding-a-primary-key)\n",
    "            - [Why do we need another primary key ?](#Why-do-we-need-another-primary-key-%3F)\n",
    "        - [Insert Records into Time Table](#Insert-Records-into-Time-Table)\n",
    "    - [#4: users Table](#%234%3A-%3Ccode%3Eusers%3C%2Fcode%3E-Table)\n",
    "        - [Extract Data for Users Table](#Extract-Data-for-Users-Table)\n",
    "        - [Manage duplicates caused by type and keep the last level (or status)](#Manage-duplicates-caused-by-type-and-keep-the-last-level-(or-status))\n",
    "        - [Choosing the SQL data type for users table](#Choosing-the-SQL-data-type-for-users-table)\n",
    "        - [Insert Records into Users Table the bulky way](#Insert-Records-into-Users-Table-the-bulky-way)        \n",
    "    - [#5: songplays Table](#%235%3A-%3Ccode%3Esongplays%3C%2Fcode%3E-Table)\n",
    "        - [Instructions](#Instructions)\n",
    "        - [Test the sql query to get song & artist ID](#Test-the-sql-query-to-get-song-%26amp%3B-artist-ID)\n",
    "        - [Choosing the SQL data type for songplay table](#Choosing-the-SQL-data-type-for-songplay-table)\n",
    "        - [Getting artists & song ID](#Getting-artists-%26amp%3B-song-ID)\n",
    "        - [Finally insert songplay data](#Finally-insert-songplay-data)\n",
    "- [Close Connection to Sparkify Database](#Close-Connection-to-Sparkify-Database)\n",
    "- [Implement etl.py](#Implement-%3Ccode%3Eetl.py%3C%2Fcode%3E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Processes tuning\n",
    "I used this notebook to develop the ETL process in `etl.py`.<br>\n",
    "It is useful for understand the reflections made for the data structure, and how data were analysed<br>\n",
    "\n",
    "## Common elements\n",
    "This section simply prepare the notebook and the database for further tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sql_queries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run create_tables.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the database and get a cursor to process SQL queries\n",
    "conn = psycopg2.connect( DSN_SPARKIFY )\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files( root_directory , file_search_query = '*' ):\n",
    "    '''\n",
    "        lookup for json file in root_directory and return a list of full file path\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            root_directory : str - filepath\n",
    "            file_search_query : str - a query string for files, will be appended to each founded folder\n",
    "        Returns\n",
    "        -------\n",
    "            list(str) : a list of full path on each files\n",
    "        Raise\n",
    "        -----\n",
    "            ValueError if filepath is empty  \n",
    "    '''\n",
    "        \n",
    "    if( root_directory == \"\" ):\n",
    "        raise ValueError('filepath must be defined')\n",
    "        \n",
    "    all_files = []\n",
    "    #iterate over the folder tree structure\n",
    "    for root, dirs, files in os.walk( root_directory ):\n",
    "        #search json files on each folder\n",
    "        files = glob.glob( os.path.join( root , file_search_query ) )\n",
    "        for f in files :\n",
    "            #concatenate result in the list\n",
    "            all_files.append(os.path.abspath(f))\n",
    "    \n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process `song_data`\n",
    "In this first part, I'll perform ETL on the first dataset, `song_data`, to create the `songs` and `artists` dimensional tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do some data analysis & quality check\n",
    "I want to get a global impression so I load all datas and use Pandas data frame tools<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all json in a dataframe\n",
    "I first do a list comprehension to get all content \n",
    "```python \n",
    "df_from_each_json = ( pd.read_json( f , lines = True ) for f in song_files )\n",
    "```\n",
    "but I need to know the file name to help me identify duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_files = get_files('data/song_data' , '*.json' )\n",
    "list_df = [] #list of dataframe\n",
    "\n",
    "#read each files and append a column with the full filename\n",
    "for f in song_files:\n",
    "    df_from_json =  pd.read_json( f , lines = True )\n",
    "    df_from_json['filename'] = f\n",
    "    list_df.append( df_from_json )\n",
    "\n",
    "#concatenate each dataframe in one\n",
    "df_songs_json_origin = pd.concat( list_df , ignore_index = True )\n",
    "\n",
    "if(  df_songs_json_origin['artist_id'].count() != len( song_files ) ):\n",
    "    raise AssertionError('It should have as many row as json files')\n",
    "else:   \n",
    "    print('Great : {} rows of data for {} files'.format( len( df_songs_json_origin.index ) , len( song_files ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for duplicates\n",
    "Pandas is a great tool :-) the describe() method help identify quickly duplicates.<br>\n",
    "Like the ids are alphanumeric (artist_id & song_id), I only include \"Object\" columns (instead of include='all')<br>\n",
    "The differnce between count & unique point to some duplication (artist_id & song_id)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs_json_origin.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's detach duplicates with the eponym method from the whole dataset<br>\n",
    "I have to remove added column (filename) to fit to the original dataset<br>\n",
    "_note : a previous iteration lead me to know about duplicated files..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs_duplicated = df_songs_json_origin[ df_songs_json_origin.drop( 'filename' , axis=1 ).duplicated( keep=False) ]\n",
    "df_songs_duplicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are these problematic files ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs_duplicated['filename'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok in fact they are no real duplicate I certainly create those file with my direct exploration of data.<br>\n",
    "But it's a good opportunity to implement and test duplication removal<br>\n",
    "For that I prefer to use pandas drop_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first remove the filename columns\n",
    "df_songs_json_origin = df_songs_json_origin.drop( 'filename' , axis=1 )\n",
    "df_songs_json = df_songs_json_origin.drop_duplicates(inplace=False)\n",
    "df_songs_json.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing SQL data type & constraint\n",
    "Now my dataset seams to be clean, I have a look at datatype to shape songs & artist tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs_json.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing really fancy, but I choose (_SQL DATATYPE for numpy datatype_):\n",
    "- TEXT for object\n",
    "- NUMERIC for float64\n",
    "- INT for int64\n",
    "\n",
    "I choose the most \"generic and appropriate\" SQL data types.<br>\n",
    "My aim is to avoid error or bad conversion at import in the ETL pipeline<br>\n",
    "I also choose artist_id & song_id as primary key in their own table, artist_id in songs table should not be null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #1: `songs` Table\n",
    "In this section I will play with different loading technique : simple insert, morgify et COPY.\n",
    "### Extract Data for Songs Table\n",
    "Songs table only expose columns song ID, title, artist ID, year, and duration<br>\n",
    "So I prepare a dataframe to fit this structure.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs = df_songs_json[['song_id','title','artist_id','year','duration']]\n",
    "df_songs.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So great : the count & unique on `song_id` confirm the primary key choice.<br>\n",
    "It seams that only one artist play two songs and each song_id are unique "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert (only one) Record into Song Table\n",
    "Implement the `song_table_insert` query in `sql_queries.py` and run the cell below to insert a record for this song into the `songs` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `songs` table in the sparkify database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute( song_table_insert, df_songs.values[0].tolist() )\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Run `test.ipynb` to see if you've successfully added a record to this table._<br>\n",
    "...sorry I don't want to leave this page... ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"SELECT * FROM songs\"\"\")\n",
    "rows = cur.fetchall()\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some assertion \n",
    "if( df_songs.values[0].tolist()[0] == rows[0][0] ):\n",
    "    print('Alright, the first row have been inserted with the key {}'.format( df_songs.values[0].tolist()[0] ))\n",
    "else:\n",
    "    raise AssertionError('There is a problem with the first cell value, expected {}, actual {}'.format( df_songs.values[0].tolist()[0], rows[0][0] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert all Record into Song Table in one transaction\n",
    "I my own experience it's better to insert all data in \"one shoot\" : life is short.<br>\n",
    "I done some investigation about the COPY command<br>\n",
    "After some research I found that executemany is inefficient with PostegreSQL (unlike MySQL)<br>\n",
    "Even psycopg documentation admit that : [Fast execution helpers](http://initd.org/psycopg/docs/extras.html#fast-exec)<br>\n",
    "I decide to have a look at 'morgify()' and the copy style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The morgify() way\n",
    "So a first solution is to build a large SQL query with morgify() method<br> \n",
    "It will construct an insert query with multiple values, like this  `INSERT INTO table_numeric (index) VALUES (1),(2),(3)` <br>\n",
    "morgify need a parameterized query like `INSERT INTO songs (song_id , title, artist_id, year , duration) VALUES  (%s, %s, %s, %s, %s)`<br>\n",
    "But it will repeat the whole query again and again. This is why I separate the queries to be able to handle that strings generation (takea a look below)<br>\n",
    "Some doc [inserting lots of data into a remote Postgres efficiently](https://nelsonslog.wordpress.com/2015/04/27/inserting-lots-of-data-into-a-remote-postgres-efficiently/)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to explain my sql_queries organisation\n",
    "print(' song_table_insert_light  : {}'.format(song_table_insert_light))\n",
    "print(' song_table_insert_params : {}'.format(song_table_insert_params))\n",
    "print(' song_table_insert        : {}'.format(song_table_insert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I already inserted the first row, so I sliced the dataframe values... but keep some for copy\n",
    "data = df_songs.values[1:3].tolist()\n",
    "\n",
    "dataText = ','.join(cur.mogrify( song_table_insert_params , row).decode('utf-8') for row in data)\n",
    "\n",
    "print( \"The query will be :\\n\"+song_table_insert_light +\"\\n\"+ dataText )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it's what I'm looking for. Let's execute that query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the insert\n",
    "cur.execute( song_table_insert_light + dataText )\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then query the songs table to display 3 rows\n",
    "cur.execute(\"\"\"SELECT * FROM songs\"\"\")\n",
    "rows = cur.fetchall()\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some assertion \n",
    "if( len(rows) != 3):\n",
    "    raise AssertionError('The number of row in db is wrong, expected {}, actual {}'.format( 3, len(rows) ) )\n",
    "elif( df_songs.values[1].tolist()[0] != rows[1][0] ):\n",
    "    raise AssertionError('There is a problem with the second cell value, expected {}, actual {}'.format( df_songs.values[1].tolist()[0], rows[1][0] ) )\n",
    "else:\n",
    "    print('Alright, 3 rows have been inserted with the key {}'.format( df_songs.values[1].tolist()[0] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this method work but we depend on a string variable.<br>\n",
    "We can only operate in memory and I'm pretty sure that we will have trouble at some point (memory available, size of the input data...)<br>\n",
    "Let's have a look at the COPY method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The COPY way\n",
    "The COPY method wich is an admin tool of PostgreSQL (https://www.postgresql.org/docs/current/sql-copy.html).<br>\n",
    "Psycopg implement an interface with `cursor.copy_from()`<br>\n",
    "I have to pass a StringIO, an in-memory stream<br>\n",
    "\n",
    "Some links :\n",
    "- [Using COPY TO and COPY FROM](http://initd.org/psycopg/docs/usage.html#using-copy-to-and-copy-from)\n",
    "- [`copy_from` documentation](http://initd.org/psycopg/docs/cursor.html#cursor.copy_from)\n",
    "- [StringIO : An in-memory stream for text I/O](https://docs.python.org/3/library/io.html#io.StringIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#core lib that contains StringIO \n",
    "import io \n",
    "\n",
    "# create a buffer for CSV infos\n",
    "buffer = io.StringIO()\n",
    "\n",
    "# I only want to process the unprocessed\n",
    "df_songs_the_rest = df_songs[3:].copy()\n",
    "\n",
    "#serialize the dataframe into the buffer (without index or header)\n",
    "df_songs_the_rest.to_csv(buffer , index=False , header=False)\n",
    "\n",
    "# I have the move the pointer at the start of the stream to do another iteration\n",
    "buffer.seek(0)\n",
    "\n",
    "#default params doesn't fit to CSV\n",
    "cur.copy_from(buffer, 'songs' , sep=',' , columns=( 'song_id', 'title', 'artist_id', 'year', 'duration') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the `copy_from` works\n",
    "cur.execute(\"\"\"SELECT * FROM songs\"\"\")\n",
    "rows = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some assertion \n",
    "if( len(rows) != 71):\n",
    "    raise AssertionError('The number of row in db is wrong, expected {}, actual {}'.format( 71, len(rows) ) )\n",
    "else:\n",
    "    print('\\n\\nAlright, 71 rows have been inserted with the key {}\\n'.format( df_songs.values[1].tolist()[0] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok I'm happy with both method, I will use mogrify() to process small amount of data (in this notebook) and copy_from for the ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #2: `artists` Table\n",
    "The previous section defined the main technique that I will used for the ETL pipeline<br>\n",
    "I will care about data only\n",
    "### Extract Data for Artists Table\n",
    "Here again i will only choose a subset of columns : artist ID, name, location, latitude, and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artist_origin = df_songs_json[['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']]\n",
    "df_artist_origin .head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artist_origin.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the duplicates\n",
    "Here my artist_id key has duplcate, first have a look on duplication and remove those rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first identify concerned rows\n",
    "df_artist_duplicated = df_artist_origin[ df_artist_origin.duplicated( keep=False) ]\n",
    "df_artist_duplicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, no doubts, I can remove thos rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artist = df_artist_origin.drop_duplicates(inplace=False)\n",
    "df_artist.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Record into Artist Table\n",
    "Implement the `artist_table_insert` query in `sql_queries.py` and run the cell below to insert a record for this song's artist into the `artists` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `artists` table in the sparkify database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(artist_table_insert, df_artist.values[0].tolist() )\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then query the artist table to display 3 rows\n",
    "cur.execute(\"\"\"SELECT * FROM artists\"\"\")\n",
    "rows = cur.fetchall()\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `test.ipynb` to see if you've successfully added a record to this table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert artist data with copy_from\n",
    "To make COPY works I had to manage several points :\n",
    "- uniformize empty value in order to consider them in copy_from()\n",
    "- define another separator (tab) in the CSV file thus I can import strings with comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace NaN of longitude / lattitude with a arbitrary NULL value\n",
    "df_artist = df_artist.fillna('NULL')\n",
    "#replace all empty cells with NULL\n",
    "df_artist= df_artist.replace('','NULL')\n",
    "#avoid usage of tab in all cells...\n",
    "df_artist = df_artist.replace('\\t','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I only want to process the unprocessed\n",
    "df_artist_the_rest = df_artist[1:].copy()\n",
    "\n",
    "# create a buffer for CSV infos\n",
    "buffer = io.StringIO()\n",
    "\n",
    "#serialize the dataframe into the buffer (without index or header and specify tab as separator)\n",
    "df_artist_the_rest.to_csv( buffer , index=False , header=False, sep='\\t' )\n",
    "\n",
    "# I have the move the pointer at the start of the stream to do another iteration\n",
    "buffer.seek(0)\n",
    "\n",
    "# do it in artist with tab as separator\n",
    "cur.copy_from(buffer, 'artists' , sep='\\t', null='NULL' , columns=( 'artist_id', 'name', 'location', 'lattitude', 'longitude' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query the whole table\n",
    "cur.execute(\"\"\"SELECT * FROM artists\"\"\")\n",
    "rows = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some assertion \n",
    "if( len(rows) != len( df_artist.index ) ):\n",
    "    raise AssertionError('The number of row in db is wrong, expected {}, actual {}'.format( len( df_artist.index ) , len(rows) ) )\n",
    "else:\n",
    "    print('\\n\\n Alright, {} artists have been inserted\\n'.format(len(rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons learned with the json files\n",
    "1. In my case, I have to deal with duplicates files.. they were created by my own with the jupyter lab :-)<br>\n",
    "2. I tested multiple methods and focused on heavy load method COPY<br>\n",
    "Working with the whole set of data, instead of one row, allow to quickly identify a lot of problem<br>\n",
    "To make it work, as far as I can see, I have to take care of NULL<br>\n",
    "3. The PostgreSQL implementation of COPY with CSV is limited to a one-byte character.<br>\n",
    "I choose tabulation for this purpose because there are no \"rich\" textual informations and little chance to meet<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process `log_data`\n",
    "In this part, I perform ETL on the second dataset, `log_data`, to create the `time` and `users` dimensional tables, as well as the `songplays` fact table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Again... do some data analysis & quality check\n",
    "With my first iteration with songs_data, I directly suspect duplicates on files but there wasn't the case (I cannot open log_data in Jupyter ;-).<br>\n",
    "Anyway I load content in a dataframe and try to remove duplicate lines<br>\n",
    "The log seems pretty good, the reason is certainly because it come from a server generated log. Those above come from human input.. trust the machine ;-)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_files = get_files('data/log_data' , '*.json' )\n",
    "\n",
    "#load each files in a data frame\n",
    "df_logs_json = ( pd.read_json( f , lines = True ) for f in log_files )\n",
    "\n",
    "#concatenate each dataframe in one\n",
    "df_logs_origin = pd.concat( df_logs_json , ignore_index = True )\n",
    "\n",
    "#drop duplicates\n",
    "df_logs = df_logs_origin.drop_duplicates(inplace=False)\n",
    "\n",
    "print( '\\n\\nThere is {} duplicate\\n'.format( len(df_logs.index) - len(df_logs_origin.index)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I directly create a dataframe with NextSong only.<br>\n",
    "The describe(all) show that a log file is well filled with the same `count` for each columns<br>\n",
    "So I don't need to wrangle data :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the content to page NextSong and create a new copy of the dataframe\n",
    "df_NextSong = df_logs[ df_logs['page'] == 'NextSong' ].copy()\n",
    "df_NextSong.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #3: `time` Table\n",
    "### Extract Data for Time Table\n",
    ">_base instructions :_\n",
    ">- Filter records by `NextSong` action\n",
    ">- Convert the `ts` timestamp column to datetime\n",
    ">  - Hint: the current timestamp is in milliseconds\n",
    ">- Extract the timestamp, hour, day, week of year, month, year, and weekday from the `ts` column and set `time_data` to a list containing these >values in order\n",
    ">  - Hint: use pandas' [`dt` attribute](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.html) to access easily datetimelike properties.\n",
    ">- Specify labels for these columns and set to `column_labels`\n",
    ">- Create a dataframe, `time_df,` containing the time data for this file by combining `column_labels` and `time_data` into a dictionary and converting this into a dataframe\n",
    "\n",
    "I do all above instructions directly in a dataframe<br>\n",
    "`df_NextSong` already adress NextSong log entries (necessity of previous section)<br>\n",
    "I add all new columns to the base dataframe to facilitate db load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the ts column to a datetime column\n",
    "df_NextSong['start_time'] = pd.to_datetime( df_NextSong['ts'], unit='ms')\n",
    "\n",
    "#append column with the correct name\n",
    "df_NextSong['hour'] = df_NextSong['start_time'].dt.hour\n",
    "df_NextSong['day'] = df_NextSong['start_time'].dt.day\n",
    "df_NextSong['week'] = df_NextSong['start_time'].dt.week\n",
    "df_NextSong['month'] = df_NextSong['start_time'].dt.month\n",
    "df_NextSong['year'] = df_NextSong['start_time'].dt.year\n",
    "df_NextSong['weekday'] = df_NextSong['start_time'].dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy_from() doesn't work at all with a timestamp including milliseconds<br>\n",
    "It seems that the momentum is fixed on .769 \\[ms\\] on every row... so I decide to remove this glitch with a tape conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the ms part of datetime with a conversion\n",
    "df_NextSong['start_time'] = df_NextSong['start_time'].astype('datetime64[s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the time table dataframe\n",
    "df_times = df_NextSong[['start_time', 'hour', 'day', 'week', 'month', 'year', 'weekday']]\n",
    "df_times.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the SQL data type for time table and adding a primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dTypes lead me simply to `int`<br>\n",
    "For `start_time` the format looks like PostgreSQL `TIMESTAMP`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we need another primary key ?\n",
    "`time` & `songs_play` table are in a 1:1 relation (linked by the log row)<br>\n",
    "But there are \"only\" 6813 unique start_time on 6820.<br>\n",
    "It means that some transactions occurs at the same time<br>   \n",
    "To avoid 1:N association, I have to add an extra column `time_id` which will be the index of the dataframe<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times.describe(include='datetime64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we have to to look in the main dataframe to prove the synchronicity of transaction.<br> \n",
    "We can see below that those duplicate rows are clearly separate transaction and clearly consecutive<br>\n",
    "\n",
    "_Thanks to [DSM on stackOverflow](https://stackoverflow.com/questions/14657241/how-do-i-get-a-list-of-all-the-duplicate-items-using-pandas-in-python) for the query below..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I limit the columns output to make it more readable\n",
    "pd.concat(g for _, g in df_NextSong.groupby(\"start_time\") if len(g) > 1)[['start_time','artist','firstName','lastName','userId']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Records into Time Table\n",
    "> _original instructions_ :<br>\n",
    "> Implement the `time_table_insert` query in `sql_queries.py` and run the cell below to insert records for the timestamps in this log file into the `time` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `time` table in the sparkify database.\n",
    ">```python\n",
    "for i, row in time_df.iterrows():\n",
    "    cur.execute(time_table_insert, list(row))\n",
    "    conn.commit()\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here I will directly implement the COPY method.<br>\n",
    "The big deal here was to understand from where the problem with copy_from()<br>\n",
    "There was no error output, so I have to found that PostgreSQL `TIMESTAMP` doesn't like Pandas `datetime64`, in particular the millisecond part<br>\n",
    "My solution was to convert `start_time` with `df_NextSong['start_time'].astype('datetime64[s]')` to remove the \\[ms\\] part ([look above](#Extract-Data-for-Time-Table))<br>\n",
    "A simpler solution could be to store start_time as TEXT in the DB... but less fun<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a buffer for CSV infos\n",
    "buffer = io.StringIO()\n",
    "\n",
    "#serialize the dataframe into the buffer with index as a key\n",
    "df_times.to_csv(buffer , index=True , header=False , sep='\\t' )\n",
    "\n",
    "# I have the move the pointer at the start of the stream to do another iteration\n",
    "buffer.seek(0)\n",
    "\n",
    "#default params doesn't fit to CSV\n",
    "cur.copy_from(buffer, 'times' , sep='\\t' , columns=( 'time_id', 'start_time', 'hour', 'day', 'week', 'month', 'year', 'weekday') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"SELECT * FROM times\"\"\")\n",
    "rows = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some assertion \n",
    "if( len(rows) != len( df_times.index ) ):\n",
    "    raise AssertionError('The number of row in db is wrong, expected {}, actual {}'.format( len( df_times.index ) , len(rows) ) )\n",
    "else:\n",
    "    print('\\n\\n Alright, {} times rows have been inserted\\n'.format(len(rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #4: `users` Table\n",
    "### Extract Data for Users Table\n",
    "- Select columns for user ID, first name, last name, gender and level and set to `df_users`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_users = df_NextSong[['userId', 'firstName', 'lastName', 'gender', 'level']]\n",
    "df_all_users.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage duplicates caused by type and keep the last level (or status)\n",
    "I can see that there are lot of duplicates. Instructions state that songplays use user ID as a key<br>\n",
    "After some trial, I found that userId can be either `str` (when level = paid) or `int` (when level = free) !<br>\n",
    "That lead to some drop_duplicates() trouble<br>\n",
    "_Note : I can report the problem about log function to the dev team ;-)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy df to be able to change the content of `user_df`\n",
    "df_users = df_all_users.copy()\n",
    "\n",
    "#convert userID to int to \n",
    "df_users['userId'] = pd.to_numeric( df_users['userId'])\n",
    "df_users.drop_duplicates(inplace=True)\n",
    "df_users.describe( include='all' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'On {} rows, only {} are unique'.format( df_users['userId'].count() , len(df_users['userId'].unique()) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df_users` doesn't have empty cells, nice<br>\n",
    "But the difference between userID count (104) and unique (96) has to be investigated<br>\n",
    "Again I reuse the \"famous\" DSM query to extract \"groupedby userID\" greater then 1<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(g for _, g in df_users.groupby(\"userId\") if len(g) > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I investigate the evolution of level in the base log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract only usefull info \n",
    "df_investigate = df_NextSong[ df_NextSong[\"userId\"].astype(str) == \"88\" ][['userId','level','start_time','artist']]\n",
    "\n",
    "#put first & last 3 rows\n",
    "pd.concat( [ df_investigate.head(3) , df_investigate.tail(3) ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not pretty sure what is the meaning of `level` column in a `users` perspective<br>\n",
    "By the way this info will be accessible in `songplays` tuple associate with the user<br>\n",
    "I interpret it like the \"current status\" of the enrollment<br>\n",
    "So I decide to keep this column but keep only the last value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid `level` in drop_duplicates()process and keep only the last one\n",
    "df_users.drop_duplicates( inplace=True, keep='last' , subset=['userId', 'firstName', 'lastName', 'gender'])\n",
    "\n",
    "#confirm the duplication eradication\n",
    "print( '\\n\\nNow, on {} rows, {} are unique\\n'.format( df_users['userId'].count() , len( df_users['userId'].unique() ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing duplicate with PostgreSQL `Upsert`\n",
    "The first review point me to the `ON CONFLICT` statment that can be really usefull<br>\n",
    "As I clean data in python (and I have to because of copy_from() usage), I could state that `ON CONFLICT DO NOTHING` is enough.. but nothing to learn<br>\n",
    "First create a use case with duplicates :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will contains a list of identical users expect on the level fields\n",
    "test_users = []\n",
    "\n",
    "# arbitrary get the first users in the df\n",
    "first_user = df_users.values[0].tolist()\n",
    "\n",
    "# append seamly users (by copy!)\n",
    "test_users.append( first_user.copy() )\n",
    "first_user[4] = 'paid1'\n",
    "test_users.append(first_user.copy() )\n",
    "first_user[4] = 'paid2'\n",
    "test_users.append(first_user.copy() )\n",
    "test_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append statment to manage conflict\n",
    "user_table_insert_conflict = user_table_insert +' ON CONFLICT(user_id) DO UPDATE set level=EXCLUDED.level'\n",
    "#print( user_table_insert_conflict )\n",
    "user_table_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the insert several times without error\n",
    "for user in test_users:\n",
    "    cur.execute( user_table_insert , user )\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('SELECT * FROM users WHERE users.user_id = {}'.format( first_user[0] ) )\n",
    "rows = cur.fetchall()\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows[0]\n",
    "# do some assertion \n",
    "if( len(rows) != 1 ):\n",
    "    raise AssertionError('The number of row in db is wrong, expected {}, actual {}'.format( 1  , len(rows) ) )\n",
    "elif( rows[0][4] != first_user[4]):\n",
    "    raise AssertionError('Upsert doesn\\'t with the level, expected {}, actual {}'.format( first_user[4] , rows[0][4]  ) )\n",
    "else:\n",
    "    print('\\n\\n Alright, level have been changed to {} \\n'.format(rows[0][4] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the row for the above processing\n",
    "cur.execute( 'DELETE FROM users WHERE users.user_id = {}'.format( first_user[0] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, ther is a good tutorial on the eponym site : [PostgreSQL Upsert Using INSERT ON CONFLICT statement](http://www.postgresqltutorial.com/postgresql-upsert/) <br>\n",
    "> as stated above, because of the usage of copy_from() and mass loading with CSV file : I have to care about duplicate before loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the SQL data type for users table\n",
    "Nothing fancy here just TEXT fields.<br>\n",
    "I can convert `userID` to `INT` to make it a `PRIMARY KEY` but should keep in mind the types mix on `songsplay`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Records into Users Table the bulky way\n",
    "> _original instructions_ :<br>\n",
    "> Implement the `user_table_insert` query in `sql_queries.py` and run the cell below to insert records for the users in this log file into the `users` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `users` table in the sparkify database.\n",
    "> ```python\n",
    ">for i, row in user_df.iterrows():\n",
    ">     cur.execute(user_table_insert, row)\n",
    ">     conn.commit()\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a buffer for CSV infos\n",
    "buffer = io.StringIO()\n",
    "\n",
    "#serialize the dataframe into the buffer\n",
    "df_users.to_csv(buffer , index=False , header=False , sep='\\t' )\n",
    "\n",
    "# move the pointer at the start of the stream to let copy_from do its job\n",
    "buffer.seek(0)\n",
    "\n",
    "#do the export\n",
    "cur.copy_from(buffer, 'users' , sep='\\t' , columns=('user_id', 'first_name', 'last_name', 'gender', 'level') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"SELECT * FROM users\"\"\")\n",
    "rows = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some assertion \n",
    "if( len(rows) != len( df_users.index ) ):\n",
    "    raise AssertionError('The number of row in db is wrong, expected {}, actual {}'.format( len( df_users.index ) , len(rows) ) )\n",
    "else:\n",
    "    print('\\n\\n Alright, {} users have been inserted\\n'.format(len(rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #5: `songplays` Table\n",
    "### Instructions\n",
    "**Extract Data and Songplays Table**\n",
    "\n",
    "This one is a little more complicated since information from the songs table, artists table, and original log file are all needed for the `songplays` table. Since the log file does not specify an ID for either the song or the artist, you'll need to get the song ID and artist ID by querying the songs and artists tables to find matches based on song title, artist name, and song duration time.\n",
    "- Implement the `song_select` query in `sql_queries.py` to find the song ID and artist ID based on the title, artist name, and duration of a song.\n",
    "- Select the timestamp, user ID, level, song ID, artist ID, session ID, location, and user agent and set to `songplay_data`\n",
    "\n",
    "**Insert Records into Songplays Table**\n",
    "- Implement the `songplay_table_insert` query and run the cell below to insert records for the songplay actions in this log file into the `songplays` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `songplays` table in the sparkify database.\n",
    "\n",
    "> Here is the original function proposed to insert songplay which isn't compatible with COPY<br>\n",
    "\n",
    "```python\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    # get songid and artistid from song and artist tables\n",
    "    cur.execute(song_select, (row.song, row.artist, row.length))\n",
    "    results = cur.fetchone()\n",
    "    \n",
    "    if results:\n",
    "        songid, artistid = results\n",
    "    else:\n",
    "        songid, artistid = None, None\n",
    "\n",
    "    # insert songplay record\n",
    "    songplay_data = ()\n",
    "    cur.execute(songplay_table_insert, songplay_data)\n",
    "    conn.commit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the sql query to get song & artist ID\n",
    "I will try to retrieve a specific entry in `df_songs` to confirm that my query is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the first entry as a list : \n",
    "# the first 3 elements are the parameters\n",
    "# the last 2 are the expected results\n",
    "test_song = list(df_songs_json[['artist_name','title','duration','song_id','artist_id']].loc()[0])\n",
    "test_song "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_select_for_test = (\"\"\"\n",
    "SELECT \n",
    "     songs.song_id, artists.artist_id\n",
    "FROM \n",
    "    songs \n",
    "INNER JOIN \n",
    "    artists ON (artists.artist_id = songs.artist_id)\n",
    "WHERE \n",
    "        artists.name = %s\n",
    "    and songs.title = %s \n",
    "    and songs.duration = %s \n",
    "\"\"\")\n",
    "\n",
    "# the first 3 elements are the parameters\n",
    "cur.execute( song_select_for_test , test_song[0:3] )\n",
    "results = cur.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some assertion \n",
    "if( results != tuple(test_song[3:]) ):\n",
    "    raise AssertionError('The SQL request is , expected {}, actual {}'.format( tuple(test_song[3:]) , results ) )\n",
    "else:\n",
    "    print('\\n\\n Alright, the query return the ids : {} which is correct\\n'.format(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the SQL data type for songplay table\n",
    "I will create a new dataframe to\n",
    "- reduce the number of columns\n",
    "- convert the userId columns\n",
    "- add a column with an index ( keep in mind the 1:1 relation with times table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the avaialble column to a new dataframe\n",
    "df_songplay = df_NextSong[['userId', 'level', 'sessionId',  'location', 'userAgent','artist','song','length']].copy()\n",
    "\n",
    "#remeber the bug between free & paid log detected in users table\n",
    "df_songplay['userId'] = pd.to_numeric( df_songplay['userId'])\n",
    "df_songplay['time_id'] = df_songplay.index\n",
    "\n",
    "df_songplay.describe(include ='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No problem with duplicate, all columns present 6820 entries<br>\n",
    "Of course song_id, artist_id should match there foreign key type (TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting artists & song ID\n",
    "The initial version consist on a for loop with directs INSERT in db (see instructions)<br>\n",
    "I want to implement the COPY method so I have to integrate this info in `df_songplays` to allow usage of to_csv() (asnd for fun ;-)<br>\n",
    "But I have slightly problem : my songs data is really poor and neither songs is present in log data !.<br>\n",
    "I want to be able to test my code : So first I have to distribute songs data on logs data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign knows values to df_songplay\n",
    "for index, row in df_songs_json[:10].iterrows():\n",
    "    df_songplay.loc[index,'artist'] =row['artist_name']\n",
    "    df_songplay.loc[index,'song'] = row['title']\n",
    "    df_songplay.loc[index,'length'] = row['duration']\n",
    "    \n",
    "df_songplay.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to take advantage on the concise notation of Pandas through apply()<br>\n",
    "So I implement a lambda that will lookup infos in the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_select_for_test_2 = (\"\"\"\n",
    "SELECT \n",
    "     songs.song_id, artists.artist_id\n",
    "FROM \n",
    "    songs \n",
    "INNER JOIN \n",
    "    artists ON (artists.artist_id = songs.artist_id)\n",
    "WHERE \n",
    "        artists.name = %s\n",
    "    and songs.title = %s \n",
    "    and songs.duration = %s\n",
    "\"\"\")\n",
    "\n",
    "def lookup_song_and_artist( params , cur , query ):\n",
    "    '''\n",
    "        >> Function to apply in a dataframe that came from a Log Dataset <<\n",
    "        Based on params[], query the DB for artist & song ID\n",
    "        Parameters\n",
    "        ----------\n",
    "            It is tricky to use to because params must be well ordered\n",
    "            params[0] : str - artist name\n",
    "            params[1] : str - song title\n",
    "            params[2] : decimal - length of songs\n",
    "            cur : cursor - the psycopg cursor used to trigger the query\n",
    "            query : str - the sql query to execute with params (to be passed bay apply(args=) )\n",
    "        Returns\n",
    "        -------\n",
    "            list(songid, artistid) : \n",
    "                - a list of related entity db primary key\n",
    "                - 'NULL', 'NULL' if nothing found\n",
    "                - 'Error' , 'exception error' in case of exceptions\n",
    "    '''\n",
    "    #concretise params\n",
    "    artist = params[0]\n",
    "    song = params[1]\n",
    "    length = params[2]\n",
    "        \n",
    "    \n",
    "    try:\n",
    "        # query the db\n",
    "        cur.execute( query , (artist, song , length))\n",
    "        results = cur.fetchone()\n",
    "    except psycopg2.Error as e:\n",
    "        # catch the error and return an empty result\n",
    "        results = 'Error' , e \n",
    "        \n",
    "    if results:\n",
    "        #ok we have a match or an error\n",
    "        songid, artistid = results\n",
    "    else:\n",
    "        #instead we nulls\n",
    "        songid, artistid = 'NULL', 'NULL'\n",
    "        \n",
    "    return ( songid, artistid )  \n",
    "\n",
    "\n",
    "df_songplay['songid'],  df_songplay['artistid'] = zip(*df_songplay[['artist','song' , 'length' ]].apply( lookup_song_and_artist , axis=1 , args=(cur, song_select_for_test_2,)))\n",
    "\n",
    "df_songplay[df_songplay['songid'] != 'NULL'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally insert songplay data\n",
    "Like other table, I will implement copy_from()<br>\n",
    "I have to rearrange `df_songplay` column to simplify the to_csv() export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songplay = df_songplay[['time_id','userId','level','songid', 'artistid','sessionId','location','userAgent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a buffer for CSV infos\n",
    "buffer = io.StringIO()\n",
    "\n",
    "#serialize the dataframe into the buffer\n",
    "df_songplay.to_csv(buffer , index=True , header=False , sep='\\t' )\n",
    "\n",
    "# move the pointer at the start of the stream to let copy_from do its job\n",
    "buffer.seek(0)\n",
    "()\n",
    "#do the export\n",
    "cur.copy_from(buffer, 'songplays' , sep='\\t' , \\\n",
    "              columns=('songplay_id', 'time_id', 'user_id', 'level', 'song_id', 'artist_id', 'session_id', 'location', 'user_agent') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"SELECT * FROM songplays\"\"\")\n",
    "rows = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some assertion \n",
    "if( len(rows) != len( df_songplay.index ) ):\n",
    "    raise AssertionError('The number of row in db is wrong, expected {}, actual {}'.format( len( df_songplay.index ) , len(rows) ) )\n",
    "else:\n",
    "    print('\\n\\n Alright, {} songplay have been inserted\\n'.format(len(rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# asking some questions\n",
    "OK so normally the full ETL pipeline has processed so the db is full of datas<br>\n",
    "I used this section to develop my dashbord queries (see `dashboard.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSELECT \\n     songplays.location\\n    , COUNT(songplays.songplay_id) as songplay_counter\\n    , COUNT(songplays.songplay_id)::decimal / 6820 * 100\\nFROM \\n    songplays\\nGROUP BY\\n    songplays.location \\nORDER BY \\n    songplay_counter DESC\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_count_songplay_by_location_test =\"\"\"\n",
    "SELECT \n",
    "     songplays.location\n",
    "    , COUNT(songplays.songplay_id) as songplay_counter\n",
    "    , COUNT(songplays.songplay_id)::decimal / {tot} * 100\n",
    "FROM \n",
    "    songplays\n",
    "GROUP BY\n",
    "    songplays.location \n",
    "ORDER BY \n",
    "    songplay_counter DESC\n",
    "\"\"\"\n",
    "query_count_songplay_by_location_test.format(tot=6820)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('San Francisco-Oakland-Hayward, CA',\n",
       "  691,\n",
       "  Decimal('10.13196480938416422300')),\n",
       " ('Portland-South Portland, ME', 665, Decimal('9.75073313782991202300')),\n",
       " ('Lansing-East Lansing, MI', 557, Decimal('8.16715542521994134900')),\n",
       " ('Chicago-Naperville-Elgin, IL-IN-WI',\n",
       "  475,\n",
       "  Decimal('6.96480938416422287400')),\n",
       " ('Atlanta-Sandy Springs-Roswell, GA', 456, Decimal('6.68621700879765395900')),\n",
       " ('Waterloo-Cedar Falls, IA', 397, Decimal('5.82111436950146627600')),\n",
       " ('Lake Havasu City-Kingman, AZ', 321, Decimal('4.70674486803519061600')),\n",
       " ('Tampa-St. Petersburg-Clearwater, FL',\n",
       "  307,\n",
       "  Decimal('4.50146627565982404700')),\n",
       " ('San Jose-Sunnyvale-Santa Clara, CA',\n",
       "  292,\n",
       "  Decimal('4.28152492668621700900')),\n",
       " ('Sacramento--Roseville--Arden-Arcade, CA',\n",
       "  270,\n",
       "  Decimal('3.95894428152492668600'))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute(query_count_songplay_by_location_test.format(tot=6820))\n",
    "rows = cur.fetchall()\n",
    "print( len(rows) )\n",
    "rows[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Connection to Sparkify Database\n",
    "conn.close()\n",
    "# conn = psycopg2.connect( DSN_SPARKIFY )\n",
    "# cur = conn.cursor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
