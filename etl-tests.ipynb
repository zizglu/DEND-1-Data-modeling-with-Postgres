{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL testing\n",
    "I use this notebook to help implement `etl.py` and markdowned `README.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README setup\n",
    "The cell below is used to fine tune the readme.md files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Purpose of this database](#Purpose-of-this-database)\n",
    "- [The ETL process](#The-ETL-process)\n",
    "- [The schema](#The-schema)\n",
    "- [How To run](#How-To-run)\n",
    "- [Files in this repository](#Files-in-this-repository)\n",
    "- [References](#References)\n",
    "\n",
    "# Purpose of this database\n",
    "\n",
    "Sparkify want to analyze how their new (web)app is used. For that, they mainly dispose of a web server log.<br>\n",
    "This log expose among other the *next_song* action which can help to determine : <br>\n",
    "- from where, when and with which web browser the app is used\n",
    "- the peak hours, how many people are using\n",
    "- user songs choice what do he like\n",
    "\n",
    "This can help improve the service provided and maybe lead to a suggestions system<br>\n",
    "On other hand, they export their songs database who can help making a match between user and artist location and maybe respond to the question \"do our customer buy locally ?\".<br>\n",
    "> The exposed songs data in this lab is very limited. I have had to insert manually data in order to test the query for songs & artist ID in \"Getting artists & song ID\" in `ETL.ipynb`\n",
    "\n",
    "# The ETL process\n",
    "\n",
    "> A lot of my thought/reflexion are in etl.ipynb (so go there please ;-)<br>\n",
    "\n",
    "The files are directly extracted to Pandas dataframes for transformation.<br>\n",
    "For loading the data into PostgreSQL, I choosed the `COPY` style option.<br>\n",
    "That implicate no INSERT queries and the whole transform process has to be done in Python with Pandas.<br>\n",
    "_So I can not use the `ON CONFLICT` statement as I dont use any direct `INSERT` queries (or UPSERT) for the loading phase_<br>\n",
    "\n",
    "The key point about transformation is to prepare to datas to direct input. Dataframe have to be CSV compliant\n",
    "So I have had to take care of : \n",
    "- remove tab character because of its usage in CSV file, \n",
    "- uniformise empty, NULL or NaN (Not a Number) \"values\" in order to make copy_from() directly\n",
    "- datatype cannot be mixed for the same column, specialy for primary keys (for example `int` & `str` in users dataset)\n",
    "- ensure that there are no duplicates\n",
    "\n",
    "I mainly used Pandas dataframe to do those jobs and also to better choose datatype in PostgreSQL<br>\n",
    "The presence of infos depend of the source of the dataset : ther are less infos in human created files then in computer generated files.<br>\n",
    "\n",
    "# The schema\n",
    "The goals mentioned above imply logically songplays as the fact table<br>\n",
    "The tables extracted from the same dataset are potentially in a 1:1 relation :\n",
    "- users, times & songplays from log json files\n",
    "- songs & artists from songs json files\n",
    "\n",
    "Some point had to be mentionned here :<br>\n",
    "- Implementation of table constraints have an impact on the sequence of creation & deletion.\n",
    "- The suggested primary key for times table (timestamp) isn't reliable.<br> \n",
    "This is generally the case when a key means something (here the timestamp)<br>\n",
    "Because an event in the log can without doubt occurs at the same time.<br>\n",
    "The solution was to implement a SERIAL PK field in times table and use it has a foreign key in somgplays table<br>\n",
    "(see \"Why do we need another primary key ?\" in etl.ipynb). \n",
    "- the COPY method lead me understand a type mixing in user_id field<br>\n",
    "So there is a potential bug in the log function of the web app !<br>\n",
    "(see \"Manage duplicates caused by type and keep the last level (or status)\" in etl.ipynb)\n",
    "\n",
    "Here you can find the schema :\n",
    "\n",
    "\n",
    "![alt text](/ERD-diagram.png \"Sparkify ERD diagram\")\n",
    "\n",
    "Direct link to [Sparkify ERD diagram created with dbdiagram.io](https://dbdiagram.io/d/5ccdf91bf7c5bb70c72fddbd)\n",
    "\n",
    "# How To run\n",
    "In a jupyter notebook cell use the following commands (test.ipynb)\n",
    "``` ipython\n",
    "%run create_tables.py #prepare the database schema\n",
    "%run etl.py #launch the ETL pipline\n",
    "```\n",
    "# Files in this repository\n",
    "> A TOC is generally implemented in each notebook\n",
    "\n",
    "- dashboard.ipynb : trigger some queries & plot related graphs\n",
    "- etl-tests.ipynb : used to faciliatte the developpment of etl.py\n",
    "- etl.ipynb : this file contains all the brainstorming and the analysis on data\n",
    "- test.ipynb : used to quickly query the db. Contains tools to mange buggy db connections\n",
    "- create_tables.py : Manage creation of the db and tables\n",
    "- etl.py : ETL pipeline\n",
    "- README.md : this file\n",
    "- sql_queries.py : list of DSN & SQL queries to create, drop, insert & select\n",
    "\n",
    "# References\n",
    "[Million SOngs Dataset](https://labrosa.ee.columbia.edu/millionsong/pages/field-list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import io\n",
    "from sql_queries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run create_tables.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect( DSN_SPARKIFY )\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files( root_directory , file_search_query = '*' ):\n",
    "    '''\n",
    "        lookup for json file in root_directory and return a list of full file path\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            root_directory : str - filepath\n",
    "            file_search_query : str - a query string for files, will be appended to each founded folder\n",
    "        Returns\n",
    "        -------\n",
    "            list(str) : a list of full path on each files\n",
    "        Raise\n",
    "        -----\n",
    "            ValueError if filepath is empty  \n",
    "    '''\n",
    "        \n",
    "    if( root_directory == \"\" ):\n",
    "        raise ValueError('filepath must be defined')\n",
    "        \n",
    "    all_files = []\n",
    "    #iterate over the folder tree structure\n",
    "    for root, dirs, files in os.walk( root_directory ):\n",
    "        #search json files on each folder\n",
    "        files = glob.glob( os.path.join( root , file_search_query ) )\n",
    "        for f in files :\n",
    "            #concatenate result in the list\n",
    "            all_files.append(os.path.abspath(f))\n",
    "    \n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe_from_jsonfiles(root_directory):\n",
    "    '''\n",
    "        Concatenante all json files founded in root_directory\n",
    "        Drop duplicate directly\n",
    "        Parameters\n",
    "        ----------       \n",
    "            root_directory str - the folder to import in a dataframe\n",
    "        Returns\n",
    "        -------        \n",
    "            a Pandas dataframe\n",
    "    '''\n",
    "    \n",
    "    #get all files\n",
    "    files = get_files(root_directory , '*.json' )\n",
    "\n",
    "    #load each files in a data frame\n",
    "    df_from_each_json = ( pd.read_json( f , lines = True ) for f in files )\n",
    "\n",
    "    #concatenate each dataframe in one\n",
    "    df_concatenated = pd.concat( df_from_each_json , ignore_index = True )\n",
    "\n",
    "    #drop duplicates\n",
    "    return df_concatenated.drop_duplicates(inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe_for_export( df , force_empty_string_to_null = True , null_string = 'NULL' , sep='\\t' , sep_replace_with=''):\n",
    "    '''\n",
    "       Prepare (&uniformise) the DF for CSV exportation\n",
    "       Replace NaN or empty cell with NULL \n",
    "        \n",
    "        Parameters\n",
    "        ----------       \n",
    "            df (Dataframe) - the Pandas dataframe\n",
    "            force_empty_string_to_null (bool) - by default empty strings are nullified\n",
    "            null_string (str) - the string used to represent NULL (by def)\n",
    "            sep (str) - to separator choosed in CSV file..\n",
    "            sep_replace_with (str) - .. to replce with\n",
    "        Returns\n",
    "        -------        \n",
    "            a Pandas dataframe\n",
    "    '''   \n",
    "    #NaN becomes NULL\n",
    "    df = df.fillna(null_string)\n",
    "    \n",
    "    #replace all empty cells with NULL\n",
    "    if( force_empty_string_to_null ): df = df.replace('', null_string)\n",
    "    \n",
    "    #avoid usage of tab in all cells...\n",
    "    df = df.replace(sep, sep_replace_with)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_song_and_artist( params , query ):\n",
    "    '''\n",
    "        >> Function to apply in a dataframe that came from a Log Dataset <<\n",
    "        Based on params[], query the DB for artist & song ID\n",
    "        Parameters\n",
    "        ----------\n",
    "            It is tricky to use to because params must be well ordered\n",
    "            params[0] : str - artist name\n",
    "            params[1] : str - song title\n",
    "            params[2] : decimal - length of songs\n",
    "            query : str - the sql query to execute with params (to be passed bay apply(args=) )\n",
    "        Returns\n",
    "        -------\n",
    "            list(songid, artistid) : \n",
    "                - a list of related entity db primary key\n",
    "                - 'NULL', 'NULL' if nothing found\n",
    "                - 'Error' , 'exception error' in case of exceptions\n",
    "    '''\n",
    "    #concretise params\n",
    "    artist = params[0]\n",
    "    song = params[1]\n",
    "    length = params[2]\n",
    "        \n",
    "    \n",
    "    try:\n",
    "        # query the db\n",
    "        cur.execute( query , (artist, song , length))\n",
    "        results = cur.fetchone()\n",
    "    except psycopg2.Error as e:\n",
    "        # catch the error and return an empty result\n",
    "        results = 'Error' , e \n",
    "        \n",
    "    if results:\n",
    "        #ok we have a match or an error\n",
    "        songid, artistid = results\n",
    "    else:\n",
    "        #instead we nulls\n",
    "        songid, artistid = 'NULL', 'NULL'\n",
    "        \n",
    "    return ( songid, artistid )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_df_to_db( cur , df , tablename , table_columns , with_index=False , separator='\\t'):\n",
    "    '''\n",
    "        Manage the df copy to db. \n",
    "        This functions manage the creation of the CSV to be imported\n",
    "        \n",
    "        Parameters\n",
    "        ----------       \n",
    "            cur - cursor to the database where the copy_from will be performed\n",
    "            df - Pandas dataframe to export to SQL\n",
    "            tablename - table where Pandas data will be imported\n",
    "            table_columns (str) - list of the sql columns in tablename\n",
    "            with_index=False - do we export the df index as first column ?\n",
    "            separator='\\t' - separator \n",
    "    '''\n",
    "    # create a buffer for CSV infos\n",
    "    buffer = io.StringIO()\n",
    "    \n",
    "    #serialize the dataframe into the buffer (no header at all)\n",
    "    df.to_csv( buffer , index=with_index , header=False, sep=separator)\n",
    "    \n",
    "    # I have the move the pointer at the start of the stream to do another iteration\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    #default params doesn't fit to CSV\n",
    "    try:\n",
    "        cur.copy_from(buffer, tablename , sep=separator , columns=table_columns , null='NULL' )\n",
    "    except psycopg2.Error as e:\n",
    "        print('Error while processing table{} : {}'.format(tablename , e ))    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# song_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs_json = create_dataframe_from_jsonfiles('data/song_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs = df_songs_json[['song_id','title','artist_id','year','duration']].copy()\n",
    "df_artist = df_songs_json[['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']].copy()\n",
    "df_artist.drop_duplicates(inplace=True)\n",
    "df_songs = clean_dataframe_for_export(df_songs)\n",
    "df_artist = clean_dataframe_for_export(df_artist)\n",
    "\n",
    "len(df_artist.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df_to_db(cur, df_artist, 'artists',( 'artist_id', 'name', 'location', 'lattitude', 'longitude' ) )\n",
    "copy_df_to_db(cur, df_songs, 'songs', ( 'song_id', 'title', 'artist_id', 'year', 'duration') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"SELECT * FROM artists\"\"\")\n",
    "\n",
    "rows = cur.fetchall()\n",
    "len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logs_json = create_dataframe_from_jsonfiles('data/log_data')\n",
    "df_NextSong = df_logs_json[ df_logs_json['page'] == 'NextSong' ].copy()\n",
    "len(df_NextSong.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the ts column to a datetime column\n",
    "df_NextSong['start_time'] = pd.to_datetime( df_NextSong['ts'], unit='ms')\n",
    "df_NextSong['start_time'] = df_NextSong['start_time'].astype('datetime64[s]')\n",
    "\n",
    "#append column needed for times table\n",
    "df_NextSong['hour'] = df_NextSong['start_time'].dt.hour\n",
    "df_NextSong['day'] = df_NextSong['start_time'].dt.day\n",
    "df_NextSong['week'] = df_NextSong['start_time'].dt.week\n",
    "df_NextSong['month'] = df_NextSong['start_time'].dt.month\n",
    "df_NextSong['year'] = df_NextSong['start_time'].dt.year\n",
    "df_NextSong['weekday'] = df_NextSong['start_time'].dt.weekday\n",
    "\n",
    "df_NextSong['userId'] = pd.to_numeric( df_NextSong['userId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## times tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times = df_NextSong[['start_time', 'hour', 'day', 'week', 'month', 'year', 'weekday']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df_to_db(cur, df_times, 'times', ( 'time_id', 'start_time', 'hour', 'day', 'week', 'month', 'year', 'weekday') , with_index=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"SELECT * FROM times\"\"\")\n",
    "\n",
    "rows = cur.fetchall()\n",
    "len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## users tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = df_NextSong[['userId', 'firstName', 'lastName', 'gender', 'level']].copy()\n",
    "#convert userID to int to \n",
    "#df_users['userId'] = pd.to_numeric( df_users['userId'])\n",
    "df_users.drop_duplicates( inplace=True, keep='last' , subset=['userId', 'firstName', 'lastName', 'gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df_to_db(cur, df_users, 'users', ('user_id', 'first_name', 'last_name', 'gender', 'level') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"SELECT * FROM users\"\"\")\n",
    "\n",
    "rows = cur.fetchall()\n",
    "len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will contains a list of identical users expect on the level fields\n",
    "test_users = []\n",
    "# arbitrary get the first users in the df\n",
    "first_user = df_users.values[0].tolist()\n",
    "# append seamly users (by copy!)\n",
    "test_users.append( first_user.copy() )\n",
    "user[4] = 'paid'\n",
    "test_users.append(first_user.copy() )\n",
    "user[4] = 'paid'\n",
    "test_users.append(first_user.copy() )\n",
    "test_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('SELECT * FROM users WHERE users.user_id = {}'.format( first_user[0] ) )\n",
    "rows = cur.fetchall()\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute( 'DELETE FROM users WHERE users.user_id = {}'.format( first_user[0] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_table_insert_conflict = user_table_insert +' ON CONFLICT(user_id) DO UPDATE set level=EXCLUDED.level'\n",
    "print( user_table_insert_conflict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in test_users:\n",
    "    cur.execute( user_table_insert_conflict, user )\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('SELECT * FROM users WHERE users.user_id = {}'.format(user[0]))\n",
    "rows = cur.fetchall()\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n",
    "conn = psycopg2.connect( DSN_SPARKIFY )\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## songplays tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songplay = df_NextSong[['userId', 'level', 'sessionId',  'location', 'userAgent','artist','song','length']].copy()\n",
    "\n",
    "#remeber the bug between free & paid log detected in users table\n",
    "#df_songplay['userId'] = pd.to_numeric( df_songplay['userId'])\n",
    "df_songplay['time_id'] = df_songplay.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookup artist & song IDs\n",
    "df_songplay['songid'],  df_songplay['artistid'] = zip(*df_songplay[['artist','song' , 'length' ]].apply( lookup_song_and_artist , axis=1 , args=(song_select,)))\n",
    "#rearrange the column\n",
    "df_songplay = df_songplay[['time_id','userId','level','songid', 'artistid','sessionId','location','userAgent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df_to_db(cur, df_songplay, 'songplays', ('songplay_id', 'time_id', 'user_id', 'level', 'song_id', 'artist_id', 'session_id', 'location', 'user_agent'), with_index=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"SELECT * FROM songplays\"\"\")\n",
    "\n",
    "rows = cur.fetchall()\n",
    "len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songplay[df_songplay['songid'] != 'NULL'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sone query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import io\n",
    "from sql_queries import *\n",
    "\n",
    "conn = psycopg2.connect( DSN_SPARKIFY )\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Creating database tables.....\n",
      "*********** Dropping tables.....\n",
      "*********** Creating tables   .....\n",
      "*********** Processing song_data.....\n",
      "*********** Processing log_data.....\n",
      "*********** Processing terminated.....\n"
     ]
    }
   ],
   "source": [
    "%run create_tables.py #prepare the database schema\n",
    "%run etl.py #launch the ETL pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, datetime.datetime(2018, 11, 9, 0, 6, 17), 0, 9, 45, 11, 2018, 4),\n",
       " (1, datetime.datetime(2018, 11, 9, 0, 9, 46), 0, 9, 45, 11, 2018, 4)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute(\"\"\"SELECT * FROM times\"\"\")\n",
    "rows = cur.fetchall()\n",
    "print( len(rows) )\n",
    "#rows[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  0,\n",
       "  42,\n",
       "  'paid',\n",
       "  None,\n",
       "  None,\n",
       "  275,\n",
       "  'New York-Newark-Jersey City, NY-NJ-PA',\n",
       "  '\"\"\"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"\"\"'),\n",
       " (1,\n",
       "  1,\n",
       "  42,\n",
       "  'paid',\n",
       "  None,\n",
       "  None,\n",
       "  275,\n",
       "  'New York-Newark-Jersey City, NY-NJ-PA',\n",
       "  '\"\"\"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"\"\"'),\n",
       " (2,\n",
       "  2,\n",
       "  42,\n",
       "  'paid',\n",
       "  None,\n",
       "  None,\n",
       "  275,\n",
       "  'New York-Newark-Jersey City, NY-NJ-PA',\n",
       "  '\"\"\"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"\"\"'),\n",
       " (3,\n",
       "  3,\n",
       "  42,\n",
       "  'paid',\n",
       "  None,\n",
       "  None,\n",
       "  275,\n",
       "  'New York-Newark-Jersey City, NY-NJ-PA',\n",
       "  '\"\"\"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"\"\"'),\n",
       " (4,\n",
       "  4,\n",
       "  42,\n",
       "  'paid',\n",
       "  None,\n",
       "  None,\n",
       "  275,\n",
       "  'New York-Newark-Jersey City, NY-NJ-PA',\n",
       "  '\"\"\"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"\"\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute(\"\"\"SELECT * FROM songplays\"\"\")\n",
    "rows = cur.fetchall()\n",
    "print( len(rows) )\n",
    "#rows[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n",
    "conn = psycopg2.connect( DSN_SPARKIFY )\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_count_user_listen =\"\"\"\n",
    "SELECT\n",
    "      times.hour\n",
    "    , COUNT( songplays.songplay_id )\n",
    "FROM songplays\n",
    "INNER JOIN times ON times.time_id = songplays.time_id\n",
    "group by times.hour\n",
    "ORDER BY times.hour\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 155),\n",
       " (1, 154),\n",
       " (2, 117),\n",
       " (3, 109),\n",
       " (4, 136),\n",
       " (5, 162),\n",
       " (6, 183),\n",
       " (7, 179),\n",
       " (8, 207),\n",
       " (9, 270),\n",
       " (10, 312),\n",
       " (11, 336),\n",
       " (12, 308),\n",
       " (13, 324),\n",
       " (14, 432),\n",
       " (15, 477),\n",
       " (16, 542),\n",
       " (17, 494),\n",
       " (18, 498),\n",
       " (19, 367),\n",
       " (20, 360),\n",
       " (21, 280),\n",
       " (22, 217),\n",
       " (23, 201)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute(query_count_user_listen)\n",
    "rows = cur.fetchall()\n",
    "print( len(rows) )\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT \n",
    "    songplays.songplay_id\n",
    "FROM songplays\n",
    "INNER JOIN times ON times.time_id = songplays.time_id\n",
    "\n",
    "SELECT \n",
    "    users.user_id\n",
    "    ,count(songplays.user_id)\n",
    "FROM users\n",
    "INNER JOIN songplays ON users.user_id = songplays.user_id\n",
    "group by users.user_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     users.user_id\n",
    "    ,users.first_name ||' '|| users.last_name\n",
    "    ,COUNT(songplays.user_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
